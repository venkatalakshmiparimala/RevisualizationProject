[
  {
    "objectID": "project_source_code.html",
    "href": "project_source_code.html",
    "title": "Project Source Codes",
    "section": "",
    "text": "Crash Clarity: Data-Driven Insights for Enhancing UK Road Safety using statistical models"
  },
  {
    "objectID": "project_source_code.html#loading-libraries",
    "href": "project_source_code.html#loading-libraries",
    "title": "Project Source Codes",
    "section": "Loading Libraries:",
    "text": "Loading Libraries:\n\n# Load required libraries\n\nsuppressWarnings({\n library(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)  \nlibrary(maps)\nlibrary(leaflet)\nlibrary(readr)\n\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(viridis) \nlibrary(rnaturalearth)  # For loading world map data\n\n\n})\n\n\n# Load necessary libraries\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Load the data\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Data Cleaning Steps\n# 1. Remove rows with missing or NA values in important columns\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(!is.na(accident_severity), \n         !is.na(weather_conditions), \n         !is.na(road_surface_conditions))\n\n# 2. Ensure the accident_severity column has valid values (1, 2, 3)\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(accident_severity %in% c(1, 2, 3))\n\n# 3. Convert necessary columns to factors\nroad_casualities$accident_severity &lt;- factor(road_casualities$accident_severity, \n                                 levels = c(1, 2, 3),\n                                 labels = c(\"Life-Threatening\", \"Significant\", \"Mild\"))\nroad_casualities$weather_conditions &lt;- as.factor(road_casualities$weather_conditions)\nroad_casualities$road_surface_conditions &lt;- as.factor(road_casualities$road_surface_conditions)\nroad_casualities$urban_or_rural_area &lt;- as.factor(road_casualities$urban_or_rural_area)\n\n# 4. Remove duplicates if any\nroad_casualities &lt;- road_casualities %&gt;% distinct()\n\n\n# Visualization with Cleaned Data\n# Assign updated custom colors for each severity level\nseverity_colors &lt;- c(\"Life-Threatening\" = \"red\", \n                     \"Significant\" = \"darkorange\", \n                     \"Mild\" = \"darkgreen\")\n\n# Create an Interactive Histogram with Updated Colors\nhistogram &lt;- ggplot(road_casualities, aes(x = accident_severity, fill = accident_severity)) +\n  geom_bar(alpha = 0.7, color = \"black\") +\n  scale_fill_manual(values = severity_colors) +\n  labs(title = \"Distribution of Accident Severity\",\n       x = \"Accident Severity\",\n       y = \"Count\") \n\n# Convert to interactive using plotly\ninteractive_histogram &lt;- ggplotly(histogram, tooltip = c(\"count\", \"x\"))\n\n# Display the interactive histogram\ninteractive_histogram\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\n\n# Load dataset\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Convert time column to a usable format (assuming time is in 24-hour format)\nroad_casualities$time_of_day &lt;- as.numeric(substr(road_casualities$time, 1, 2))\n\n# Define time bands based on STATS20 guidance\nroad_casualities$time_band &lt;- cut(\n  road_casualities$time_of_day,\n  breaks = c(-1, 5, 9, 15, 19, 23),\n  labels = c(\"Night (Midnight to 5 AM)\", \n             \"Morning Rush Hour\", \n             \"Daytime\", \n             \"Evening Rush Hour\", \n             \"Night (8 PM to 11 PM)\")\n)\n\n# Aggregate the data by time bands\ntime_band_summary &lt;- road_casualities %&gt;%\n  group_by(time_band) %&gt;%\n  summarise(total_accidents = n()) %&gt;%\n  arrange(desc(total_accidents))\n\n# Create a ggplot object with gradient colors\ntime_band_plot &lt;- ggplot(time_band_summary, aes(x = time_band, y = total_accidents, fill = total_accidents)) +\n  geom_bar(stat = \"identity\", color = \"black\", alpha = 0.8) +\n  labs(\n    title = \"Accidents by Time Bands (STATS20)\",\n    x = \"Time Band\",\n    y = \"Number of Accidents\"\n  ) +\n  scale_fill_gradient(low = \"pink\", high = \"red\", name = \"Total Accidents\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n# Convert the plot to interactive using plotly\ninteractive_time_band_plot &lt;- ggplotly(time_band_plot, tooltip = c(\"x\", \"y\"))\n\n# Display the interactive plot\ninteractive_time_band_plot\n\n\n\n\n\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\n\n# Define light condition mapping (STATS20)\nlight_conditions_map &lt;- c(\n  \"1\" = \"Daylight\",\n  \"4\" = \"Dark (street lights present and lit)\",\n  \"5\" = \"Dark (street lights present but unlit)\",\n  \"6\" = \"Dark (no street lights)\",\n  \"7\" = \"Other\"\n)\n\n# Map light conditions\nroad_casualities$light_conditions &lt;- as.factor(\n  light_conditions_map[as.character(road_casualities$light_conditions)]\n)\n\n# Ensure weather conditions are mapped\nweather_conditions_map &lt;- c(\n  \"1\" = \"Fine without high winds\",\n  \"2\" = \"Raining without high winds\",\n  \"3\" = \"Snowing without high winds\",\n  \"4\" = \"Fine with high winds\",\n  \"5\" = \"Raining with high winds\",\n  \"6\" = \"Snowing with high winds\",\n  \"7\" = \"Fog or mist\",\n  \"8\" = \"Other\",\n  \"9\" = \"Unknown\"\n)\n\nroad_casualities$weather_conditions &lt;- as.factor(\n  weather_conditions_map[as.character(road_casualities$weather_conditions)]\n)\n\n# Create summary data for heatmap\nheatmap_data &lt;- road_casualities %&gt;%\n  group_by(weather_conditions, light_conditions) %&gt;%\n  summarise(total_accidents = n(), .groups = \"drop\")\n\n# Create the heatmap with data labels and contrasting colors\nheatmap_plot &lt;- ggplot(heatmap_data, aes(x = weather_conditions, y = light_conditions, fill = total_accidents)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = total_accidents), color = \"black\", size = 3) +  # Add data labels\n  scale_fill_gradient(low = \"#f9f9f9\", high = \"#d73027\", name = \"Total Accidents\") +\n  labs(\n    title = \"Accidents by Weather and Light Conditions\",\n    x = \"Weather Conditions\",\n    y = \"Light Conditions\"\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 10),\n    axis.text.y = element_text(size = 10),\n    plot.title = element_text(size = 14, face = \"bold\"),\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8)\n  )\n\n# Make the heatmap interactive\ninteractive_heatmap &lt;- ggplotly(heatmap_plot, tooltip = c(\"x\", \"y\", \"fill\"))\ninteractive_heatmap\n\n\n\n\n\n`\n\n# Load the dataset\nroad_casualities &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\n# Define STATS19 road surface condition mapping\nroad_conditions_map &lt;- c(\n  \"1\" = \"Dry\",\n  \"2\" = \"Wet or damp\",\n  \"3\" = \"Snow\",\n  \"4\" = \"Frost or ice\",\n  \"5\" = \"Flood\",\n  \"6\" = \"Oil or diesel\",\n  \"7\" = \"Mud\",\n  \"8\" = \"Other\",\n  \"9\" = \"Unknown\"\n)\n\n# Step 1: Filter valid road_surface_conditions values (1 to 9)\nroad_casualities &lt;- road_casualities %&gt;%\n  filter(road_surface_conditions %in% c(1:9))\n\n# Step 2: Map road conditions to descriptions\nroad_casualities$road_conditions_desc &lt;- as.factor(\n  road_conditions_map[as.character(road_casualities$road_surface_conditions)]\n)\n\n# Debugging: Check unique values in road_conditions_desc\nprint(\"Mapped descriptions:\")\n\n[1] \"Mapped descriptions:\"\n\nprint(unique(road_casualities$road_conditions_desc))\n\n[1] Wet or damp  Dry          Unknown      Frost or ice Snow        \n[6] Flood       \nLevels: Dry Flood Frost or ice Snow Unknown Wet or damp\n\n# Step 3: Aggregate data by road surface condition\nroad_summary &lt;- road_casualities %&gt;%\n  group_by(road_conditions_desc) %&gt;%\n  summarise(total_accidents = n()) %&gt;%\n  arrange(desc(total_accidents))\n\n# Debugging: Check aggregated data\nprint(\"Aggregated road surface condition data:\")\n\n[1] \"Aggregated road surface condition data:\"\n\nprint(road_summary)\n\n# A tibble: 6 × 2\n  road_conditions_desc total_accidents\n  &lt;fct&gt;                          &lt;int&gt;\n1 Dry                            72752\n2 Wet or damp                    26944\n3 Unknown                         1617\n4 Frost or ice                    1461\n5 Snow                             241\n6 Flood                            179\n\n# Step 4: Create the bar plot\nroad_plot &lt;- ggplot(road_summary, aes(\n  x = reorder(road_conditions_desc, -total_accidents), \n  y = total_accidents, \n  fill = road_conditions_desc, \n  text = paste0(road_conditions_desc, \": \", total_accidents, \" accidents\")\n)) +\n  geom_bar(stat = \"identity\", alpha = 0.8, color = \"black\") +\n  labs(\n    title = \"Distribution of Accidents by Road Surface Conditions (STATS19)\",\n    x = \"Road Surface Conditions\",\n    y = \"Number of Accidents\"\n  ) +\n  scale_fill_manual(values = c(\n    \"Dry\" = \"red\",\n    \"Wet or damp\" = \"#73a2c6\",\n    \"Snow\" = \"#1b9e77\",\n    \"Frost or ice\" = \"#d95f02\",\n    \"Flood\" = \"#7570b3\",\n    \"Oil or diesel\" = \"#e7298a\",\n    \"Mud\" = \"#66a61e\",\n    \"Other\" = \"#e6ab02\",\n    \"Unknown\" = \"#a6761d\"\n  )) \n\n# Convert the plot to an interactive plot\ninteractive_road_plot &lt;- ggplotly(road_plot, tooltip = \"text\")\n\n# Display the interactive plot\ninteractive_road_plot\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(caret)\n# Load necessary libraries\nlibrary(nnet)  # For multinomial logistic regression\nlibrary(caret) # For data partitioning\n\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n\n# Select relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(\"accident_severity\", \"weather_conditions\")])\n\n# Convert columns to factors\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\ndata_cleaned$weather_conditions &lt;- as.numeric(data_cleaned$weather_conditions)\n\n# Split the dataset into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train multinomial logistic regression model\nmultinom_model &lt;- multinom(accident_severity ~ weather_conditions, data = train_data)\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n# Summarize the model\nsummary(multinom_model)\n\nCall:\nmultinom(formula = accident_severity ~ weather_conditions, data = train_data)\n\nCoefficients:\n  (Intercept) weather_conditions\n2     2.64414         0.06126746\n3     3.76240         0.12285242\n\nStd. Errors:\n  (Intercept) weather_conditions\n2  0.04240724         0.02156981\n3  0.04154713         0.02120177\n\nResidual Deviance: 100785 \nAIC: 100793 \n\n# Make predictions\npredictions &lt;- predict(multinom_model, newdata = test_data)\n\n# Evaluate the model\nconfusion_matrix &lt;- table(test_data$accident_severity, predictions)\nprint(\"Confusion Matrix:\")\n\n[1] \"Confusion Matrix:\"\n\nprint(confusion_matrix)\n\n   predictions\n        1     2     3\n  1     0     0   304\n  2     0     0  4687\n  3     0     0 15859\n\n# Calculate accuracy\naccuracy &lt;- sum(diag(confusion_matrix)) / sum(confusion_matrix)\nprint(paste(\"Accuracy:\", round(accuracy, 4)))\n\n[1] \"Accuracy: 0.7606\"\n\n\n\n# Load necessary libraries\nlibrary(nnet)  # For multinomial logistic regression\nlibrary(caret) # For data partitioning\nlibrary(pROC)  # For AUC and ROC curves\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Select relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(\"accident_severity\", \"weather_conditions\")])\n\n# Convert columns to factors and numeric types\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\ndata_cleaned$weather_conditions &lt;- as.numeric(data_cleaned$weather_conditions)\n\n# Split the dataset into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train multinomial logistic regression model\nmultinom_model &lt;- multinom(accident_severity ~ weather_conditions, data = train_data)\n\n# weights:  9 (4 variable)\ninitial  value 91633.053773 \niter  10 value 50406.679444\nfinal  value 50392.493956 \nconverged\n\n# Predict probabilities for the test data\nprobabilities &lt;- predict(multinom_model, newdata = test_data, type = \"probs\")\n\n# Create one-vs-all ROC curves for each class\nroc_curve_1 &lt;- roc(as.numeric(test_data$accident_severity == 1), probabilities[, 1], plot = TRUE, col = \"red\", main = \"ROC Curve for Multinomial Logistic Regression\")\nroc_curve_2 &lt;- roc(as.numeric(test_data$accident_severity == 2), probabilities[, 2], plot = TRUE, col = \"blue\", add = TRUE)\nroc_curve_3 &lt;- roc(as.numeric(test_data$accident_severity == 3), probabilities[, 3], plot = TRUE, col = \"green\", add = TRUE)\n\n# Add legend\nlegend(\"bottomright\", legend = c(\"Class 1 (Fatal)\", \"Class 2 (Serious)\", \"Class 3 (Slight)\"),\n       col = c(\"red\", \"blue\", \"green\"), lwd = 2)\n\n\n\n\n\n\n\n# Compute AUC for each class\nauc_1 &lt;- auc(roc_curve_1)\nauc_2 &lt;- auc(roc_curve_2)\nauc_3 &lt;- auc(roc_curve_3)\n\n\n# Load necessary libraries\n# Load necessary libraries\nlibrary(randomForest)\nlibrary(pROC)\nlibrary(caret)\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Select relevant features and target variable\nfeatures &lt;- c(\"weather_conditions\", \"number_of_vehicles\", \"road_surface_conditions\", \"urban_or_rural_area\")\ntarget &lt;- \"accident_severity\"\n\n# Filter data to include only relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(features, target)])\n\n# Convert target variable to factor for classification\ndata_cleaned$accident_severity &lt;- as.factor(data_cleaned$accident_severity)\n\n# Split data into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$accident_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train a Random Forest model\nrf_model &lt;- randomForest(\n  accident_severity ~ .,\n  data = train_data,\n  ntree = 100,\n  importance = TRUE\n)\n\n# Print model summary\nprint(rf_model)\n\n\nCall:\n randomForest(formula = accident_severity ~ ., data = train_data,      ntree = 100, importance = TRUE) \n               Type of random forest: classification\n                     Number of trees: 100\nNo. of variables tried at each split: 2\n\n        OOB estimate of  error rate: 23.95%\nConfusion matrix:\n  1  2     3  class.error\n1 0  0  1218 1.0000000000\n2 0  6 18745 0.9996800171\n3 0 15 63424 0.0002364476\n\n# Predict probabilities on the test set\nrf_probabilities &lt;- predict(rf_model, newdata = test_data, type = \"prob\")\n\n# Draw AUC Curve for each class using one-vs-all approach\nauc_values &lt;- list()\nfor (i in 1:ncol(rf_probabilities)) {\n  class_label &lt;- colnames(rf_probabilities)[i]\n  roc_curve &lt;- roc(as.numeric(test_data$accident_severity == class_label), \n                   rf_probabilities[, i], \n                   plot = TRUE, \n                   main = paste(\"ROC Curve for Class\", class_label), \n                   col = i)\n  auc_values[[class_label]] &lt;- auc(roc_curve)\n  print(paste(\"AUC for Class\", class_label, \":\", round(auc(roc_curve), 4)))\n}\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 1 : 0.4996\"\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 2 : 0.5012\"\n\n\n\n\n\n\n\n\n\n[1] \"AUC for Class 3 : 0.5012\"\n\n# Visualize feature importance\nvarImpPlot(rf_model)"
  },
  {
    "objectID": "project_source_code.html#only-logistic-regression-with-2-variables",
    "href": "project_source_code.html#only-logistic-regression-with-2-variables",
    "title": "Project Source Codes",
    "section": "only Logistic regression with 2 variables",
    "text": "only Logistic regression with 2 variables\n\n# Load necessary libraries\nlibrary(nnet)    # For logistic regression\nlibrary(pROC)    # For ROC curve and AUC\nlibrary(caret)   # For data partitioning\n\n# Read the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Select relevant columns and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, c(\"accident_severity\", \"urban_or_rural_area\", \"weather_conditions\")])\n\n# Create a binary target variable (1 for Slight, 0 for Fatal and Serious)\ndata_cleaned$binary_severity &lt;- ifelse(data_cleaned$accident_severity == 3, 1, 0)\n\n# Convert necessary columns to numeric/factor\ndata_cleaned$binary_severity &lt;- as.factor(data_cleaned$binary_severity)\ndata_cleaned$urban_or_rural_area &lt;- as.numeric(data_cleaned$urban_or_rural_area)\ndata_cleaned$weather_conditions &lt;- as.numeric(data_cleaned$weather_conditions)\n\n# Split the dataset into training and testing sets\nset.seed(42)\ntrain_index &lt;- createDataPartition(data_cleaned$binary_severity, p = 0.8, list = FALSE)\ntrain_data &lt;- data_cleaned[train_index, ]\ntest_data &lt;- data_cleaned[-train_index, ]\n\n# Train logistic regression model\nlogistic_model &lt;- glm(binary_severity ~ urban_or_rural_area + weather_conditions, \n                      data = train_data, \n                      family = binomial)\nprint(logistic_model)\n\n\nCall:  glm(formula = binary_severity ~ urban_or_rural_area + weather_conditions, \n    family = binomial, data = train_data)\n\nCoefficients:\n        (Intercept)  urban_or_rural_area   weather_conditions  \n            1.60582             -0.40781              0.06153  \n\nDegrees of Freedom: 83406 Total (i.e. Null);  83404 Residual\nNull Deviance:      91810 \nResidual Deviance: 91030    AIC: 91040\n\n# Predict probabilities for the test data\nprobabilities &lt;- predict(logistic_model, newdata = test_data, type = \"response\")\n\n# Create the ROC curve\nroc_curve &lt;- roc(test_data$binary_severity, probabilities, plot = TRUE, col = \"blue\", \n                 main = \"ROC Curve for Binary Classification\")\n\n\n\n\n\n\n\n# Compute and print AUC\nauc_value &lt;- auc(roc_curve)\nprint(paste(\"AUC Value:\", round(auc_value, 4)))\n\n[1] \"AUC Value: 0.5498\"\n\n# Add thresholds to the ROC plot\nplot(roc_curve, print.auc = TRUE, col = \"blue\")"
  },
  {
    "objectID": "project_source_code.html#logistic-regression-lasso-wirth-2-combination-variables",
    "href": "project_source_code.html#logistic-regression-lasso-wirth-2-combination-variables",
    "title": "Project Source Codes",
    "section": "Logistic regression Lasso wirth 2 combination variables",
    "text": "Logistic regression Lasso wirth 2 combination variables\n\nlibrary(glmnet)\n\n# Prepare data for glmnet\nX &lt;- as.matrix(train_data[, c(\"urban_or_rural_area\", \"weather_conditions\")])\ny &lt;- as.numeric(train_data$binary_severity) - 1\n\n# Train Lasso logistic regression\nlasso_model &lt;- cv.glmnet(X, y, family = \"binomial\", alpha = 1)\n\n# Predict probabilities for test data\ntest_X &lt;- as.matrix(test_data[, c(\"urban_or_rural_area\", \"weather_conditions\")])\nlasso_probabilities &lt;- predict(lasso_model, newx = test_X, s = \"lambda.min\", type = \"response\")\n\n# Evaluate using ROC and AUC\nlasso_roc &lt;- roc(as.numeric(test_data$binary_severity), lasso_probabilities, plot = TRUE, col = \"blue\",\n                 main = \"ROC Curve for Lasso Logistic Regression\")\n\n\n\n\n\n\n\nlasso_auc &lt;- auc(lasso_roc)\nprint(paste(\"Lasso Logistic Regression AUC:\", round(lasso_auc, 4)))\n\n[1] \"Lasso Logistic Regression AUC: 0.5514\""
  },
  {
    "objectID": "project_source_code.html#logistic-regression-lasso-wirth-more-variables",
    "href": "project_source_code.html#logistic-regression-lasso-wirth-more-variables",
    "title": "Project Source Codes",
    "section": "Logistic regression Lasso wirth more variables",
    "text": "Logistic regression Lasso wirth more variables\n\n# Load necessary libraries\nlibrary(glmnet)\nlibrary(pROC)\n\n# Load the dataset\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Final project\\\\casualty_statistics.csv\")\n\n# Define selected features and target variable\nselected_features &lt;- c(\"accident_severity\", \"number_of_vehicles\", \"road_surface_conditions\", \n                       \"weather_conditions\", \"urban_or_rural_area\", \"special_conditions_at_site\", \n                       \"day_of_week\", \"time\", \"junction_detail\", \"speed_limit\")\n\n# Check if selected features exist in the dataset\nif (!all(selected_features %in% colnames(data))) {\n  missing_features &lt;- selected_features[!selected_features %in% colnames(data)]\n  stop(paste(\"The following features are missing from the dataset:\", paste(missing_features, collapse = \", \")))\n}\n\n# Filter the dataset for selected features and remove rows with missing values\ndata_cleaned &lt;- na.omit(data[, selected_features])\n\n# Create a binary target variable for classification\n# Severe (1) = accident_severity 1 or 2, Slight (0) = accident_severity 3\ndata_cleaned$binary_severity &lt;- ifelse(data_cleaned$accident_severity == 3, 0, 1)\n\n# Feature engineering: Convert 'time' into 'time_of_day' (e.g., Morning, Afternoon, Night)\ndata_cleaned$time &lt;- as.numeric(sub(\"^(\\\\d{2}):.*$\", \"\\\\1\", data_cleaned$time)) # Extract hour\ndata_cleaned$time_of_day &lt;- cut(data_cleaned$time, \n                                breaks = c(-1, 6, 12, 18, 24), \n                                labels = c(\"Night\", \"Morning\", \"Afternoon\", \"Evening\"))\n\n# Drop unnecessary columns (original 'time' and 'accident_severity')\ndata_cleaned &lt;- data_cleaned[, !(names(data_cleaned) %in% c(\"time\", \"accident_severity\"))]\n\n# Convert categorical variables into factors\ncategorical_vars &lt;- c(\"road_surface_conditions\", \"weather_conditions\", \"urban_or_rural_area\", \n                      \"special_conditions_at_site\", \"day_of_week\", \"time_of_day\", \"junction_detail\")\ndata_cleaned[categorical_vars] &lt;- lapply(data_cleaned[categorical_vars], as.factor)\n\n# Prepare data for glmnet\nX &lt;- model.matrix(binary_severity ~ ., data_cleaned)[, -1] # Remove intercept\ny &lt;- data_cleaned$binary_severity\n\n# Split data into training and testing sets\nset.seed(42)\ntrain_index &lt;- sample(1:nrow(X), size = 0.8 * nrow(X))\ntrain_X &lt;- X[train_index, ]\ntrain_y &lt;- y[train_index]\ntest_X &lt;- X[-train_index, ]\ntest_y &lt;- y[-train_index]\n\n# Train Lasso Logistic Regression model\nlasso_model &lt;- cv.glmnet(train_X, train_y, family = \"binomial\", alpha = 1)\n\n# Predict probabilities for test data\nlasso_probabilities &lt;- predict(lasso_model, newx = test_X, s = \"lambda.min\", type = \"response\")\n\n# Evaluate using ROC and AUC\nlasso_roc &lt;- roc(test_y, lasso_probabilities, plot = TRUE, col = \"blue\",\n                 main = \"ROC Curve for Lasso Logistic Regression\")\n\n\n\n\n\n\n\nlasso_auc &lt;- auc(lasso_roc)\nprint(paste(\"Lasso Logistic Regression AUC:\", round(lasso_auc, 4)))\n\n[1] \"Lasso Logistic Regression AUC: 0.612\"\n\n# Feature importance (coefficients)\ncoefficients &lt;- coef(lasso_model, s = \"lambda.min\")\nprint(\"Selected Features and Coefficients:\")\n\n[1] \"Selected Features and Coefficients:\"\n\nprint(coefficients)\n\n48 x 1 sparse Matrix of class \"dgCMatrix\"\n                                      s1\n(Intercept)                 -1.159394968\nnumber_of_vehicles          -0.256295633\nroad_surface_conditions1     0.937290733\nroad_surface_conditions2     0.925376470\nroad_surface_conditions3     0.958261001\nroad_surface_conditions4     0.708252313\nroad_surface_conditions5     0.944823220\nroad_surface_conditions9     0.445159967\nweather_conditions2         -0.069904189\nweather_conditions3         -0.117426522\nweather_conditions4          0.155343359\nweather_conditions5         -0.040067944\nweather_conditions6         -0.075081224\nweather_conditions7         -0.284458507\nweather_conditions8         -0.169295042\nweather_conditions9         -0.383972781\nurban_or_rural_area1        -0.112744643\nurban_or_rural_area2         0.086384660\nurban_or_rural_area3         0.973339463\nspecial_conditions_at_site0 -0.306314034\nspecial_conditions_at_site1 -0.507191456\nspecial_conditions_at_site2  0.314806500\nspecial_conditions_at_site3 -0.115356865\nspecial_conditions_at_site4 -0.412954815\nspecial_conditions_at_site5 -0.116574223\nspecial_conditions_at_site6 -0.353961785\nspecial_conditions_at_site7 -0.336745861\nspecial_conditions_at_site9 -1.031307541\nday_of_week2                -0.136292827\nday_of_week3                -0.175948703\nday_of_week4                -0.120039691\nday_of_week5                -0.124894449\nday_of_week6                -0.104736531\nday_of_week7                -0.002913686\njunction_detail0             0.091600324\njunction_detail1            -0.453624650\njunction_detail2            -0.188097911\njunction_detail3             .          \njunction_detail5            -0.201307564\njunction_detail6             0.008137821\njunction_detail7             .          \njunction_detail8             .          \njunction_detail9             0.001650131\njunction_detail99           -1.819330521\nspeed_limit                  0.007150706\ntime_of_dayMorning          -0.318457391\ntime_of_dayAfternoon        -0.265087170\ntime_of_dayEvening          -0.113424581"
  },
  {
    "objectID": "project_source_code.html#loading-libraries-1",
    "href": "project_source_code.html#loading-libraries-1",
    "title": "Project Source Codes",
    "section": "Loading Libraries:",
    "text": "Loading Libraries:\n\n# Load required libraries\n\nsuppressWarnings({\n library(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)  \nlibrary(maps)\nlibrary(leaflet)\nlibrary(readr)\n\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(viridis) \nlibrary(rnaturalearth)  # For loading world map data\n\n\n})"
  },
  {
    "objectID": "project_source_code.html#average-suicidal-rates-by-country-from-1950-to-2022",
    "href": "project_source_code.html#average-suicidal-rates-by-country-from-1950-to-2022",
    "title": "Project Source Codes",
    "section": "Average Suicidal Rates By Country from 1950 to 2022:",
    "text": "Average Suicidal Rates By Country from 1950 to 2022:\n\n# Load the data (change the path to where your file is stored)\nsuicide_data &lt;- read.csv(\"suicide-rates-all.csv\")\n\n# Summarize the data to get the average suicide rate for each country\naverage_suicide_rate &lt;- suicide_data %&gt;%\n  group_by(Country) %&gt;%\n  summarise(Average_Suicide_Rate = mean(suscide.rate, na.rm = TRUE), .groups = \"drop\")  # Add .groups = \"drop\" to avoid warnings\n\n# Load world map data\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Merge world map data with average suicide rates\nworld_data &lt;- merge(world, average_suicide_rate, by.x = \"name\", by.y = \"Country\", all.x = TRUE)\n\n# Create color palette for suicide rates\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = world_data$Average_Suicide_Rate, na.color = \"transparent\")\n\n# Create interactive leaflet map\nleaflet(world_data) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(Average_Suicide_Rate),\n    weight = 1,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlight = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE\n    ),\n    label = ~paste(name, \"Average Suicide Rate: \", round(Average_Suicide_Rate, 2)),\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(pal = pal, values = ~Average_Suicide_Rate, opacity = 0.7, title = \"Average Suicide Rate per 100,000\", position = \"bottomright\") %&gt;%\n  addControl(\"&lt;strong&gt;Average Suicide Rates by Country of all the ages over the years 1950 to 2022&lt;/strong&gt;\", position = \"topright\", \n             className = \"map-title\")  # Add title to the map\n\n\n\n\n\n\nAverage Suicide Rates of all the ages by Country for the top rated year: 1982\n\n# Load the data (change the path to where your file is stored)\nsuicide_data &lt;- read.csv(\"suicide-rates-all.csv\")\n\n# Calculate the overall average suicide rate for each year across all countries\nyearly_avg_suicide_rate &lt;- suicide_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Average_Suicide_Rate = mean(suscide.rate, na.rm = TRUE))\n\n# Find the year with the highest overall average suicide rate\ntop_year &lt;- yearly_avg_suicide_rate %&gt;%\n  filter(Average_Suicide_Rate == max(Average_Suicide_Rate)) %&gt;%\n  pull(Year)\n\n# Filter the data to include only the data for the top year\ntop_year_data &lt;- suicide_data %&gt;%\n  filter(Year == top_year)\n\n# Load world map data\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Merge world map data with suicide rate data for the top year\nworld_data &lt;- merge(world, top_year_data, by.x = \"name\", by.y = \"Country\", all.x = TRUE)\n\n# Create color palette for the suicide rates in the top year\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = world_data$suscide.rate, na.color = \"transparent\")\n\n# Create interactive leaflet map\nmap &lt;- leaflet(world_data) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(suscide.rate),\n    weight = 1,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlight = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE\n    ),\n    label = ~paste(name, \"Suicide Rate: \", round(suscide.rate, 2)),\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(pal = pal, values = ~suscide.rate, opacity = 0.7, title = paste(\"Suicide Rate per 100,000 in \", top_year), position = \"bottomright\")%&gt;%\naddControl(\"&lt;strong&gt;Average Suicide Rates of all the ages by Country for the top rated year: 1982&lt;/strong&gt;\", position = \"topright\", \n             className = \"map-title\")  # Add title to the map\n\n# Show the map\nmap\n\n\n\n\n\n\n\nAverage Suicide Rates by Year all over the World:\n\n# Read the data\ndata &lt;- read_csv(\n  \"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Website project\\\\website\\\\suicide-rates-all.csv\",\n  col_types = cols(\n    Country = col_character(),\n    Code = col_character(),\n    Year = col_double(),\n    `suscide-rate` = col_double()\n  )\n)\n\n# Step 1: Calculate the average suicide rate for each year\navg_suicide_by_year &lt;- data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Average_suicide_rate = round(mean(`suscide-rate`, na.rm = TRUE), 2), .groups = \"drop\")\n\n# Step 2: Create the frequency polygon\nfrequency_polygon &lt;- ggplot(avg_suicide_by_year, aes(x = Year, y = Average_suicide_rate)) +\n  geom_line(stat = \"identity\", color = \"blue\", size = 1) +  # Line to connect data points\n  geom_point(color = \"red\") +  # Points for each year\n  labs(title = \"Suicide Rates trend of all the ages and Years all over the world\", \n       x = \"Year\", \n       y = \"Average Suicide Rate per 100,000\") +\n  \n  theme(\n    plot.title = element_text(face = \"bold\"), \n    axis.title.x = element_text(face = \"bold\"), \n    axis.title.y = element_text(face = \"bold\")\n  )\n\n# Convert the frequency polygon to an interactive plot\ninteractive_frequency_polygon &lt;- ggplotly(frequency_polygon)\n\n# Show the interactive frequency polygon\ninteractive_frequency_polygon"
  },
  {
    "objectID": "project_source_code.html#top-5-years-with-highest-suicide-rates-in-top-5-countries",
    "href": "project_source_code.html#top-5-years-with-highest-suicide-rates-in-top-5-countries",
    "title": "Project Source Codes",
    "section": "Top 5 Years with Highest Suicide Rates in Top 5 Countries:",
    "text": "Top 5 Years with Highest Suicide Rates in Top 5 Countries:\n\n# Load the data (update the path to your CSV file accordingly)\nsuicide_data &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Website project\\\\website\\\\suicide-rates-all.csv\")\n\n# Calculate the average suicide rate per country and filter top 5 countries\ntop_5_countries &lt;- suicide_data %&gt;%\n  group_by(Country) %&gt;%\n  summarise(Average_Suicide_Rate = mean(suscide.rate, na.rm = TRUE)) %&gt;%\n  top_n(5, wt = Average_Suicide_Rate)\n\n# Find the top 5 years for each of the top 5 countries\ntop_5_years_per_country &lt;- suicide_data %&gt;%\n  filter(Country %in% top_5_countries$Country) %&gt;%\n  group_by(Country, Year) %&gt;%\n  summarise(Suicide_Rate = mean(suscide.rate, na.rm = TRUE)) %&gt;%\n  arrange(Country, desc(Suicide_Rate)) %&gt;%\n  group_by(Country) %&gt;%\n  slice_max(Suicide_Rate, n = 5)\n\n# Create a horizontal bar plot with increased bar size\np_bar &lt;- ggplot(top_5_years_per_country, aes(x = reorder(Country, Suicide_Rate), y = Suicide_Rate, fill = Year, text = paste(\"Country:\", Country, \"&lt;br&gt;Year:\", Year, \"&lt;br&gt;Suicide Rate:\", round(Suicide_Rate, 2)))) +\n  geom_bar(stat = \"identity\", position = position_dodge(), width = 0.9) +  # Adjust width to increase bar size\n  scale_fill_gradient(low = \"#FF9999\", high = \"#CC0000\") +  # Light to dark red gradient\n  labs(title = \"Top 5 Countries with Highest Suicide Rates and Their Top 5 Years\",\n       x = \"Suicide Rate\",\n       y = \"Country\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5),\n        legend.position = \"none\") +\n  coord_flip() +  # Flip the axes to create horizontal bars\n  \n  # Add year labels to the bars\n  geom_text(aes(label = Year), \n            position = position_dodge(width = 0.9),  # Ensure labels match bar positions\n            hjust = -0.2,   # Adjust horizontal position of text\n            size = 3,       # Set a smaller text size\n            fontface = \"bold\")       # Set a smaller text size\n\n# Convert ggplot to interactive plotly plot\ninteractive_bar_plot &lt;- ggplotly(p_bar, tooltip = \"text\")\n\n# Display the interactive plot\ninteractive_bar_plot"
  },
  {
    "objectID": "project_source_code.html#average-suicide-rates-for-ages-15-19-over-the-years",
    "href": "project_source_code.html#average-suicide-rates-for-ages-15-19-over-the-years",
    "title": "Project Source Codes",
    "section": "Average Suicide Rates for Ages 15-19 over the years:",
    "text": "Average Suicide Rates for Ages 15-19 over the years:\n\n# Load the CSV file\nchildren_data &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Website project\\\\website\\\\suicide-rates-by-age.csv\")\n\n# Step 1: Rename columns to appropriate labels\ncolnames(children_data) &lt;- c(\"Country\", \"Code\", \"Year\", \n                              \"Death_rate_15_19\", \"Death_rate_20_24\", \"Death_rate_25_29\", \n                              \"Death_rate_30_34\", \"Death_rate_35_39\", \"Death_rate_40_44\", \n                              \"Death_rate_45_49\", \"Death_rate_50_54\", \"Death_rate_55_59\", \n                              \"Death_rate_60_64\", \"Death_rate_65_69\", \"Death_rate_70_74\", \n                              \"Death_rate_75_79\", \"Death_rate_80_84\", \"Death_rate_over_85\")\n\n# Step 2: Calculate the average suicide rate for children (using 'Death_rate_15_19' as an example)\navg_suicide_rate &lt;- children_data %&gt;%\n  group_by(Country) %&gt;%\n  summarise(AverageRate = round(mean(Death_rate_15_19, na.rm = TRUE),2))\n\n# Step 3: Load map data\nworld_map &lt;- map_data(\"world\")\n\n# Step 4: Merge average suicide rates with the map data\nmap_data &lt;- world_map %&gt;%\n  left_join(avg_suicide_rate, by = c(\"region\" = \"Country\")) # Ensure to match the correct column names\n\n# Step 5: Create the static map using ggplot2\ngg &lt;- ggplot(data = map_data, aes(x = long, y = lat, group = group, fill = AverageRate)) +\n  geom_polygon(color = \"black\", aes(text = paste(\"Country:\", region, \"&lt;br&gt;Average Suicide Rate (Ages 15-19):\", round(AverageRate, 2)))) +\n  scale_fill_gradient(low = \"lightblue\", high = \"red\", na.value = \"grey50\", \n                      name = \"Average Suicide Rate\\n(Ages 15-19)\") +\n  labs(title = \"Average Suicide Rate of Children by Country (Ages 15-19)\",\n       x = \"Longitude\", \n       y = \"Latitude\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n# Step 6: Convert ggplot to an interactive plotly object\ninteractive_map &lt;- ggplotly(gg, tooltip = \"text\")\n\n# Step 7: Display the interactive map\ninteractive_map\n\n\n\n\n\n\nAverage Suicide Rate for the Top 20 Nations in the 15-19 Age Group (Across All Years):\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\n# Load the data\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Website project\\\\website\\\\suicide-rates-by-age.csv\")\n\n# Clean column names for easier access\ncolnames(data) &lt;- c(\"Entity\", \"Code\", \"Year\", \n                    \"Death_rate_15_19\", \"Death_rate_20_24\", \"Death_rate_25_29\", \n                    \"Death_rate_30_34\", \"Death_rate_35_39\", \"Death_rate_40_44\", \n                    \"Death_rate_45_49\", \"Death_rate_50_54\", \"Death_rate_55_59\", \n                    \"Death_rate_60_64\", \"Death_rate_65_69\", \"Death_rate_70_74\", \n                    \"Death_rate_75_79\", \"Death_rate_80_84\", \"Death_rate_over_85\")\n\n# Filter the data for the 15-19 age group and remove missing values\ndata_filtered &lt;- data %&gt;%\n  filter(!is.na(Death_rate_15_19))\n\n# Calculate the average suicide rate for each country across all years\navg_suicide_rate &lt;- data_filtered %&gt;%\n  group_by(Entity) %&gt;%\n  summarise(suscide_Rate = round(mean(Death_rate_15_19, na.rm = TRUE), 2)) %&gt;%\n  arrange(desc(suscide_Rate)) %&gt;%\n  top_n(20, suscide_Rate)\n\n# Create the bar plot with ggplot\nbar_plot &lt;- ggplot(avg_suicide_rate, aes(x = reorder(Entity, suscide_Rate), \n                                           y = suscide_Rate, \n                                           fill = suscide_Rate,\n                                           text = paste(\"Country:\", Entity, \"&lt;br&gt;Average Suicide Rate:\", suscide_Rate))) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +  # Flip coordinates to make country names readable\n  scale_fill_gradient(low = \"#ffcccc\", high = \"#990000\") +  # Light red to dark red gradient\n  labs(title = \"Suicide Rates Among 15-19 Year Olds in the Top 20 Nations (All Years)\", \n       x = \"Country\", \n       y = \"Average Suicide Rate per 100,000 Population\") +\n  theme(axis.text.y = element_text(angle = 0, hjust = 1, size = 8)) +  # Adjust text angle and size\n  guides(fill = guide_legend(title = \"Suicide Rate per 100,000\"))  # Change the legend title\n\n# Convert to an interactive plot with plotly\ninteractive_plot &lt;- ggplotly(bar_plot, tooltip = \"text\")  # Use the 'text' aesthetic for tooltip\n\n# Show the interactive plot\ninteractive_plot"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "Venkata Lakshmi Parimala Pasupuleti\n\n\nSoftware Engineer | Web & Database Development Specialist\nI have over seven years of experience in web development and database engineering, and my career has been built on a passion for creating high-quality, efficient solutions in dynamic work environments. I enjoy collaborating with teams and contributing to projects that solve real-world problems, especially in the areas of database design, scalable systems, and cloud-based architecture.\nI have experience in property management software development with Entrata India, where I worked within an Agile Scrum framework. I actively participated in sprint planning, project execution, and project management. I collaborated closely with cross-functional teams to deliver solutions that enhanced product performance and met client requirements. From optimizing SQL queries to developing scalable microservices, I consistently focused on improving system efficiency and addressing complex technical challenges.\nOne of the highlights of my experience was developing a microservices-based architecture that improved system scalability by 50%, and reduced deployment time by 30%, utilizing technologies such as Docker and Kubernetes. I was also deeply involved in migrating thousands of client documents from local servers to AWS cloud storage, which resolved critical storage limitations and ensured data security and accessibility.\nI led several database projects under the guidance of a database architect with 21 years of experience, gaining in-depth knowledge in database management and architecture design. Utilizing these skills, I contributed to building database structures for various projects and also served as a virtual database administrator for the product. In this role, I delivered multiple database optimization solutions to enhance product efficiency.\nIn addition to my technical expertise, I have worked on multiple projects in the affordable housing sector. I designed solutions for resident management, rental subsidy calculations, eligibility determination, and rent calculations. My contributions to the development of these systems ensured that they were reliable, secure, and user-friendly. I also worked on encrypting sensitive resident data ensuring full compliance with data protection standards.\n\n\n\nExperience\n\nSenior Software Engineer, Entrata India (Db Xento Systems) – Pune, India Jan 2023 – Aug 2024\n\nParticipated in Agile Scrum processes, leading to Sprint planning & story point estimation using Planning Poker and the Fibonacci sequence.\nOptimized database performance by designing and refining SQL queries, implementing caching strategies, and utilizing triggers, log tables, and audit tables to improve efficiency and scalability.\nInvolved in the team of database architecture and made crucial decisions while developing complex database project schemas.\nDeveloped and mentored web applications development projects using, php, and AWS services, focusing on cloud-based architecture to enhance deployment and system performance.\nBuilt end-to-end software solutions for property management and affordable housing, including rental subsidies, rent calculations, ledger management, and payment processing features, ensuring compliance with housing regulations.\nLed the development and deployment of a microservices architecture using Docker , achieving a 50% improvement in system scalability and a 30% reduction in deployment time.\nLed the bug monitoring, reduced the bug count, and maintained the bug count to less than 10 for up to 6 months.\nManaged financial functionalities, including HAP (Housing Assistance Payments) allocation, rent schedules, gross rent updates, tax credits, HUD (Housing and Urban Development) payments, subsidies, and income limits.\nEnsured compliance with federal and state regulations for housing subsidies and income restrictions.\n\n\n\nSoftware Engineer, Entrata India (Db Xento Systems) – Pune, India Oct 2018 – Dec 2022\n\nAssisted, Developed and deployed complex web designs for property management and rental solutions using PHP for backend and Multiple front-end technologies.\nDeveloped the php unit tests and helped the product to achieve the performance improvement.\nImproved application performance by optimizing SQL queries by learning query optimization techniques with collaboration of data architects and implementing indexing and database partitioning.\nCollaborated closely with cross-functional teams in an Agile environment, participating in Sprint planning, & daily standups.\nDone Peer code review and helped maintain the quality of the code.\nImproved the debugging techniques and attended to the most of the urgent bugs and resolved them for clients reducing system downtime by\nidentifying and resolving critical issues resulting in receiving appreciation badges from the manager.\n\n\n\nAssociate Software Engineer, Entrata India – Pune, India Jul 2017 – Sep 2018\n\nAssisted in developing and maintaining full-stack applications using HTML,JavaScript, AJAX, jQuery, PHP, and PostgresSQL.\nDeveloped and deployed minor modules for the property management\nParticipated in bug fixing, code debugging, and performance boosts.\nGained hands-on experience in Web development,Agile methodologies, actively involved in Sprint planning, and daily standups.\n\n\n\nTrainee, Entrata India (Db Xento Systems) Pune, India Jan 2017 – Jun 2017\n\nAcquired foundational skills in software development by learning key technologies such as PHP Concepts, Database technoques, and UI design and development.\nWorked closely with senior developers to understand Agile practices, participating in Sprint meetings and learning the fundamentals of version control using Git.\nCompleted comprehensive training in object-oriented programming (OOP) principles and database management, laying the groundwork.\n\n\n\nTechnical Skills\n\nDatabases: PostgreSQL, MySQL\n\nBackend Development: PHP, Python\n\nFrontend Technologies: AJAX, HTML, JavaScript, jQuery\n\nCloud Technologies: AWS, Docker, Kubernetes\n\nFrameworks: MVC, Singleton, Fusebox\n\nAgile Tools: Jira, ClickUp\n\nVersion Control: GIT, SVN\nProgramming and Operating Systems: PHP, Python, R , Object Oriented Programming (OOPS), Unit tests, Web Services, AJAX, HTML, JavaScript, CSS, jQuery, MVC, Design patterns, Fusebox, Unix, Windows, Linux\n\nTesting: PHP Unit, Test Driven Development (TDD), or Behavior Driven Design (BDD), Selenium Automation\nDatabases & Storage: Postgres, MySQL, Oracle\n\nCloud Computing concepts:RDS ,AWS (S3, EC2),Data Glue\n\nDistributed Technologies: Hive\n\nEnvironment Management: Docker or Containers\n\nCI/CD : GitHub\n\nReporting Tools : PowerBI\n\nWeb Frameworks: Basic NodeJS\n\nLibraries/Frameworks: PyTorch, NumPy, Pandas Methodologies: Data Analysis. Data Modeling\n\nVersion Control, Task &project Management & Performance Monitoring: SVN, Git, JIRA, ClickUp, New Relic, Agile methodologies, Waterfall,scrum\nDesigning: Adobe Acrobat DC Pro, Figma, Adobe Photoshop\n\nQueue Management: RabbitMQ \n\nEmbedded Systems and Debugging: Basic knowledge of embedded processors and real-time operating systems, excellent debugging skills \n\n\n\nORGANIZATION PROJECTS\n\nImporting Income Limits and Module Development\n\nProcessed 1.6 million records of HUD and Tax Credit income limits across five years by designing generic SQL scripts for importing and structuring data.\nAutomated workflows for eligibility definition based on family size, income levels, and percentage thresholds.\nTechnologies: PHP, Postgres SQL, PLSQL, GIT, Excel.\n\n\n\nResidents’ Sensitive Data Encryption\n\nEncrypted 100% of resident data, including phone numbers and SSNs, across the entire database using Libsodium encryption.\nUpdated over 50% of the product code to ensure encryption compliance during data insertion and retrieval.\nTechnologies: PHP, Libsodium, Postgres SQL, PLSQL, GIT.\n\n\n\nState-Wise Certification Document Development\n\nCreated 56 state-specific certification documents with merge fields for affordable housing certifications.\nIntegrated document storage and retrieval via AWS S3, enabling seamless client access.\nTechnologies: PHP, Postgres SQL, AWS S3, Adobe Acrobat DC Pro, GIT.\n\n\n\nClient Document Migration to AWS\n\nMigrated thousands of client documents from server storage to AWS S3, resolving 90% space constraints on legacy systems.\nConducted database data creation, physical document migration, and comprehensive UI testing.\nTechnologies: PHP, Postgres SQL, AWS S3, GIT.\n\n\n\nService-Based Architecture Revamp\n\nConverted 20% of product modules into a service-based architecture, ensuring scalability and efficient data sharing across products.\nImproved modularity and reduced interdependencies in the product logic.\nTechnologies: PHP, Postgres SQL, AWS S3, GIT.\n\n\n\nDatabase Architecture Overhaul for Client Documents\n\nRedesigned outdated document storage architecture, migrating hundreds of thousands of records to a modern structure.\nProvided post-deployment support for six months, stabilizing the ecosystem and resolving client-specific issues.\nTechnologies: PHP, Postgres SQL, Lucidchart, Adobe Acrobat DC Pro, GIT.\n\n\n\nRedesigning Income Limits Module\n\nApplied deep learning frameworks and Python to automate data processing workflows, leveraging API calls and OpenAI models to replace manual Excel-based operations.\nImplemented a custom neural network architecture inspired by literature to optimize the transformation and validation of complex datasets.\nDemonstrated clear communication by articulating technical concepts and project outcomes to stakeholders. \nTechnologies: Python, OpenAI, PyTorch,Pandas, Postgres SQL, GIT.\n\n\n\nMicroservices for Financial Operations\n\nDesigned microservices to handle critical financial functionalities such as rent schedules, gross rent updates, and HUD-compliant tax credit calculations.\nIntegrated these services into the main product to ensure compliance with state and federal regulations.\n\n\n\nDebugging and Performance Optimization Tools\n\nDeveloped internal debugging tools to identify and resolve system bottlenecks, reducing debugging time by 40%.\nImproved application performance through SQL query optimization, reducing response times across high-traffic modules.\n\n\n\n\nACADEMIC PROJECTS\n\nCrashClarity Project\n\nAnalyzed traffic accident data using R to identify patterns and contributing factors.\nApplyed statistical methods and visualization techniques for Machine Learning Predictive Modeling.\nPresented insights via an interactive website hosted on GitHub Pages, enhancing data accessibility and decision-making.\nTechnologies: R-programming,Statistical methods, GIT.\n\n\n\n\nEducation\n\nMaster’s in Data Analytics Engineering\nGeorge Mason University, Virginia, USA (2024 - 2026)\n\nGPA: 3.78\n\nBachelor of Technology in Computer Science & Technology\nSree Venkateshwara College of Engineering, Nellore, India (2013 - 2017)"
  },
  {
    "objectID": "project_details.html",
    "href": "project_details.html",
    "title": "Projects",
    "section": "",
    "text": "Crash Clarity: Data-Driven Insights for Enhancing UK Road Safety using statistical models\nABSTRACT\nRoad accidents and safety remain critical public health concerns worldwide, with significant societal, economic, and emotional impacts. In the United Kingdom, the government provides comprehensive data on road accidents through its Road Accident and Safety Statistics guidance. This academic project leverages these statistics to analyze and interpret the trends, patterns, and contributing factors associated with road accidents in the UK.\nThe study explores key variables such as accident severity, weather and road conditions, time of day, and demographic factors, providing actionable insights into the circumstances under which accidents are most likely to occur. Utilizing advanced data visualization techniques, including interactive heatmaps and histograms, the project presents complex information in a clear and engaging manner to enhance understanding and foster data-driven decision-making.\nThe findings emphasize the critical role of environmental and behavioral factors in road safety and aim to support policymakers, researchers, and road users in designing effective interventions to reduce accidents and improve safety measures. This project underscores the importance of leveraging statistical data to promote evidence-based strategies for safer transportation systems.\nVisualizing Accident Severity Distribution\nThis interactive histogram presents a comprehensive analysis of road accident severity levels categorized as Life-Threatening, Significant, and Mild. Each category is visually distinguished using a specific color palette, with red denoting Life-Threatening accidents, orange representing Significant accidents, and green for Mild cases. The visualization provides an intuitive understanding of the frequency distribution of these severity levels, enabling researchers and policymakers to identify patterns and focus on mitigating the most critical accident types. The interactivity of the graph allows for an in-depth examination of accident counts, enhancing data-driven decision-making and supporting evidence-based road safety interventions.\nTemporal Analysis of Road Accidents by Time Bands\nThis interactive visualization categorizes road accidents into five time bands: “Night (Midnight to 5 AM),” “Morning Rush Hour,” “Daytime,” “Evening Rush Hour,” and “Night (8 PM to 11 PM)” using STATS20 guidance. The bar plot highlights accident frequencies with a gradient color scheme, showing the highest occurrences during “Daytime” and “Evening Rush Hour”.\nThese insights help identify high-risk periods, enabling policymakers and researchers to develop targeted road safety strategies. The interactive design allows for detailed exploration of accident patterns.\nAccidents by Weather and Light Conditions\nThis interactive heatmap analyzes the influence of weather and light conditions on road accidents, highlighting combinations like “Fine without high winds” and “Daylight” with the highest frequencies. A gradient color scale emphasizes accident intensity, with data labels providing exact counts. The visualization aids in identifying high-risk conditions to inform targeted safety measures.\nImpact of Weather Conditions on Road Accidents\nThis interactive bar plot presents the distribution of road accidents under various weather conditions, highlighting categories such as Fine without high winds Raining without high winds, and Fog or mist. The gradient color scale, ranging from light pink to deep red, emphasizes the frequency of accidents, with higher counts visually more prominent. Tooltips provide precise accident counts for each weather condition, enhancing the interpret ability of the data.\nThe visualization reveals that the majority of accidents occur under Fine without high winds, suggesting that favorable weather does not necessarily mitigate risk. Such insights are critical for policymakers and researchers to understand environmental influences on road safety and to develop targeted prevention strategies.\nAccidents by Road Surface Conditions\nThis interactive bar plot examines the distribution of road accidents across various surface conditions based on STATS19 classifications, such as Dry, Wet or damp, and Snow. Each condition is color-coded for clarity, with tooltips providing detailed accident counts for enhanced interpretability.\nThe analysis reveals that the majority of accidents occur on Dry surfaces, followed by Wet or damp conditions, while adverse surfaces like Flood and Mud show significantly lower frequencies. These findings emphasize the need to consider surface conditions when implementing road safety measures, particularly for common scenarios like wet or dry roads. The visualization supports data-driven strategies for reducing accidents under diverse environmental conditions."
  },
  {
    "objectID": "project_details.html#re-visualization-project-using-r-language-introduction",
    "href": "project_details.html#re-visualization-project-using-r-language-introduction",
    "title": "Projects",
    "section": "Re-visualization Project Using R-language Introduction:",
    "text": "Re-visualization Project Using R-language Introduction:\nSuicide is the act of intentionally causing one’s own death. It can be due to many conditions or the situations. It includes Mental disorders, physical disorders, and substance abuse are the risk factors. Suicides resulted in 828,000 deaths globally in 2015, an increase from 712,000 deaths in 1990. This makes suicide the 10th leading cause of death worldwide. Every death from suicide is a tragedy.\nThe below is the Visualization on Suicides by Saloni Dattani, Lucas Rodes-Guirao, Hannah Ritchie, Max Roser, and Esteban Ortiz-Ospina. The research shows that suicide rates can be reduced with greater understanding and support. To do that the researchers considered or recognized suicide as a public health problem, and people should know that it can be prevented and its rates can be reduced."
  },
  {
    "objectID": "project_details.html#please-watch-the-full-project-presentation-on-the-following-youtube-video",
    "href": "project_details.html#please-watch-the-full-project-presentation-on-the-following-youtube-video",
    "title": "Projects",
    "section": "Please watch the full project presentation on the following YouTube video",
    "text": "Please watch the full project presentation on the following YouTube video\n\n\nOLD VISUALIZATION:\n\n\n\nSuicide rates vary around the world:\nSuicide rates vary widely between the countries. The given visualization depicts the data of annual suicide rates per 100,000 people from 1950 to 2022, across various countries. Researchers used line graph to predict the data.\n\nX-axis represents the years from 1950 to 2022 and y-axis represents the suicide rate per 100000 people, ranging from 0 to 40. It also says that higher the value, the greater will be the number of suicide rates.\nEach line of the graph represents the countries. The countries which have higher suicide rates are represented on the top. The legends taken are countries.\n\nObservations:\n\nThere is a wide range of variations between the countries. Countries like Lithuania, South Korea shows the highest suicide rates, as indicated by their position near the top of the graph.\nSome countries shows large fluctuations in the suicide rates while other countries shows the constant rate throughout the years.\nIt also says that suicide deaths are under-reported in many countries due to social stigma and culture or legal concerns means that actual rates can be higher than the reported rates.\nThe data is collected based on the data listed in the death certificates. It can impact the accuracy of the data\nThe data is adjusted for age standardization allowing a fair comparison between the countries with different age structures, ensuring that population age distribution doesn’t skew the data.\n\n\nBad Visualization Predictions:\n\nMore number of lines: The graph contains a huge number of lines which are representing the countries. This creates a messy graph it is very difficult to predict the data immediately as we look into the graph.\nColor Categorization: All the countries represented with different colors but for some countries there are distinct colors where it will be very difficult to categorize the data. There are similar colors in for different countries. We can use more contrasting colors to represent the data or we can group the colors into regions or categories.\nInteractive Labeling: With so many lines we cannot identify the particular country instantly and it is impossible to find the particular country and there are all the countries mentioned in the legend where it is impossible to identify the specific country. Hence we can use interactive Labeling for highlighting the particular country.\nNo Highlights on the key insights: All the lines in the graph are in equal size where there is no differentiation between the countries. We can highlight the countries which have highest suicide rates and lowest suicide rates with different dimensions of the lines.\n\n\nThe above visualization tells us about the reported suicide rates by age in the United States.\nObservations:\nIt explains the breakdown for the rate of suicides for different age groups like children, adults. The data highlights the trends such as the increasing or decreasing risk of suicide within the specific age over time and across different regions.\n\nIt shows the data for the suicide rates per 100,000 people across different age groups. Age specific data usually reveals trends showing which age group are more vulnerable to suicide in different regions.\nAccording to the graph it predicts that the old generation people have the higher suicide rates (Age between 80-84). The age between 15-19 suicidal rates are less. But there is growing concern about suicidal rates in young adults particularly due to health conditions and mental stress.\n\n\n\nBad Visualization Predictions:\n\nMore number of lines: The graph contains a huge number of lines which are representing the different age groups. This creates a messy graph it is very difficult to predict the data immediately as we look into the graph.\nColor Categorization: All the age groups are represented with different colors but for some there are distinct colors where it will be very difficult to categorize the data. There are similar colors for different age groups. We can use more contrasting colors to represent the data or we can group the colors into categories or age groups.\nLegend: The legend have too many entries where it is difficult for the user to identify the particular data of the age group in a particular year. Viewers must constantly shift their focus on the legend and the graph simultaneously where it would be difficult for predicting the exact information.\nLack of data insights: There is no contextual information or annotations on the graph to explain significant spikes, trends or sudden drops in the suicide rates for certain age groups.\nInteractive Labeling: Adding the interactvite labeling helps to improve the readability."
  },
  {
    "objectID": "project_details.html#re-visualizations",
    "href": "project_details.html#re-visualizations",
    "title": "Projects",
    "section": "Re-Visualizations:",
    "text": "Re-Visualizations:\n\nAccording to the above research and bad visualizations found we have made some changes and re-visualized the data as below:\nEach map or graph in this project displays suicide rates per 100,000 people to enhance the clarity and effectiveness of the visualization and this is the standard that data analysts generally follow while visualizing death related data."
  },
  {
    "objectID": "project_details.html#average-suicidal-rates-by-country-from-1950-to-2022",
    "href": "project_details.html#average-suicidal-rates-by-country-from-1950-to-2022",
    "title": "Projects",
    "section": "Average Suicidal Rates By Country from 1950 to 2022:",
    "text": "Average Suicidal Rates By Country from 1950 to 2022:\nThe map below illustrates the average suicide rates by country from 1950 to 2022, broken down by different age groups such as children, young adults, and adults across all nations.\nIn the previous visualization, the data was presented in a line graph for all countries, resulting in a cluttered and hard-to-read display. To improve clarity, we have re-visualized the data by focusing on the average suicide rates from 1950 to 2022 using a world map. In the provided dataset, we calculated the average suicide rates over the years and made predictions based on that data. The world map offers an easier and more intuitive way to interpret the data. This updated map is also interactive, allowing users to highlight specific parameters and explore the average range of deaths by suicide more flexibly.\nBased on the predictions shown in the map, Russia has the highest average suicide rates."
  },
  {
    "objectID": "project_details.html#top-5-years-with-highest-suicide-rates-in-top-5-countries",
    "href": "project_details.html#top-5-years-with-highest-suicide-rates-in-top-5-countries",
    "title": "Projects",
    "section": "Top 5 Years with Highest Suicide Rates in Top 5 Countries:",
    "text": "Top 5 Years with Highest Suicide Rates in Top 5 Countries:\nThe graph below shows the top 5 years with the highest suicide rates in the top 5 countries. First, we identified the 5 countries with the highest average suicide rates. After filtering the data to include only these countries, we selected the top 5 years with the highest suicide rates for each. Using this categorized data, we created a bar graph with ggplot. Additionally, we added interactive labeling to enhance accessibility and provide a more user-friendly experience for viewers."
  },
  {
    "objectID": "project_details.html#average-suicide-rates-for-ages-15-19-over-the-years",
    "href": "project_details.html#average-suicide-rates-for-ages-15-19-over-the-years",
    "title": "Projects",
    "section": "Average Suicide Rates for Ages 15-19 over the years:",
    "text": "Average Suicide Rates for Ages 15-19 over the years:\nThe graph below displays the average suicide rates for individuals aged 15-19. The previous visualization focused on overall suicide rates across all years and age groups. For this re-visualization, we specifically selected the 15-19 age group, as it marks the end of teen years. We used a world map to represent the data and incorporated interactive labeling for easier interpretation.\n\n\n\n\n\n\n\nAverage Suicide Rate for the Top 20 Nations in the 15-19 Age Group (Across All Years):\nThe graph below illustrates the average suicide rate for the top 20 countries in the 15-19 age group over the years 1950 to 2021. It highlights the top 20 countries for this age group, with the addition of interactive labeling for enhanced user experience."
  },
  {
    "objectID": "Visualization_project.html#running-code",
    "href": "Visualization_project.html#running-code",
    "title": "Re-Visualization Project",
    "section": "Running Code",
    "text": "Running Code\nWhen you click the Render button a document will be generated that includes both content and the output of embedded code. You can embed code like this:\n\n# Load required libraries\n\nsuppressWarnings({\n library(tidyverse)\nlibrary(ggplot2)\nlibrary(plotly)  \nlibrary(maps)\nlibrary(leaflet)\nlibrary(readr)\n\n  library(plotly)\n  library(dplyr)\n  library(viridis)  \n\n})\n\n\n# Load necessary libraries\nlibrary(dplyr)  # For data manipulation\nlibrary(leaflet)  # For creating interactive maps\nlibrary(rnaturalearth)  # For world map data\n\n# Load the data (change the path to where your file is stored)\nsuicide_data &lt;- read.csv(\"suicide-rates-all.csv\")\n\n# Summarize the data to get the average suicide rate for each country\naverage_suicide_rate &lt;- suicide_data %&gt;%\n  group_by(Country) %&gt;%\n  summarise(Average_Suicide_Rate = mean(suscide.rate, na.rm = TRUE), .groups = \"drop\")  # Add .groups = \"drop\" to avoid warnings\n\n# Load world map data\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Merge world map data with average suicide rates\nworld_data &lt;- merge(world, average_suicide_rate, by.x = \"name\", by.y = \"Country\", all.x = TRUE)\n\n# Create color palette for suicide rates\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = world_data$Average_Suicide_Rate, na.color = \"transparent\")\n\n# Create interactive leaflet map\nleaflet(world_data) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(Average_Suicide_Rate),\n    weight = 1,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlight = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE\n    ),\n    label = ~paste(name, \"Average Suicide Rate: \", round(Average_Suicide_Rate, 2)),\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(pal = pal, values = ~Average_Suicide_Rate, opacity = 0.7, title = \"Average Suicide Rate\", position = \"bottomright\") %&gt;%\n  addControl(\"&lt;strong&gt;Average Suicide Rates by Country&lt;/strong&gt;\", position = \"topright\", \n             className = \"map-title\")  # Add title to the map\n\n\n\n\n\n\n# Load the data (change the path to where your file is stored)\nsuicide_data &lt;- read.csv(\"suicide-rates-all.csv\")\n\n# Calculate the overall average suicide rate for each year across all countries\nyearly_avg_suicide_rate &lt;- suicide_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Average_Suicide_Rate = mean(suscide.rate, na.rm = TRUE))\n\n# Find the year with the highest overall average suicide rate\ntop_year &lt;- yearly_avg_suicide_rate %&gt;%\n  filter(Average_Suicide_Rate == max(Average_Suicide_Rate)) %&gt;%\n  pull(Year)\n\n# Filter the data to include only the data for the top year\ntop_year_data &lt;- suicide_data %&gt;%\n  filter(Year == top_year)\n\n# Load world map data\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Merge world map data with suicide rate data for the top year\nworld_data &lt;- merge(world, top_year_data, by.x = \"name\", by.y = \"Country\", all.x = TRUE)\n\n# Create color palette for the suicide rates in the top year\npal &lt;- colorNumeric(palette = \"YlOrRd\", domain = world_data$suscide.rate, na.color = \"transparent\")\n\n# Create interactive leaflet map\nmap &lt;- leaflet(world_data) %&gt;%\n  addTiles() %&gt;%\n  addPolygons(\n    fillColor = ~pal(suscide.rate),\n    weight = 1,\n    opacity = 1,\n    color = \"white\",\n    dashArray = \"3\",\n    fillOpacity = 0.7,\n    highlight = highlightOptions(\n      weight = 5,\n      color = \"#666\",\n      dashArray = \"\",\n      fillOpacity = 0.7,\n      bringToFront = TRUE\n    ),\n    label = ~paste(name, \"&lt;br&gt; Suicide Rate: \", round(suscide.rate, 2)),\n    labelOptions = labelOptions(\n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n      textsize = \"15px\",\n      direction = \"auto\"\n    )\n  ) %&gt;%\n  addLegend(pal = pal, values = ~suscide.rate, opacity = 0.7, title = paste(\"Suicide Rate in\", top_year), position = \"bottomright\")%&gt;%\naddControl(\"&lt;strong&gt;Average Suicide Rates of all the ages by Country for the top rated year: 1982&lt;/strong&gt;\", position = \"topright\", \n             className = \"map-title\")  # Add title to the map\n\n# Show the map\nmap\n\n\n\n\n\n\n# Read the data\ndata &lt;- read_csv(\n  \"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Website project\\\\website\\\\suicide-rates-all.csv\",\n  col_types = cols(\n    Country = col_character(),\n    Code = col_character(),\n    Year = col_double(),\n    `suscide-rate` = col_double()\n  )\n)\n\n# Step 1: Calculate the average suicide rate for each year\navg_suicide_by_year &lt;- data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Average_suicide_rate = round(mean(`suscide-rate`, na.rm = TRUE), 2), .groups = \"drop\")\n\n# Step 2: Create the frequency polygon\nfrequency_polygon &lt;- ggplot(avg_suicide_by_year, aes(x = Year, y = Average_suicide_rate)) +\n  geom_line(stat = \"identity\", color = \"blue\", size = 1) +  # Line to connect data points\n  geom_point(color = \"red\") +  # Points for each year\n  labs(title = \"Average Suicide Rates by Year all over the world\", \n       x = \"Year\", \n       y = \"Average Suicide Rate per 100,000\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\"), \n    axis.title.x = element_text(face = \"bold\"), \n    axis.title.y = element_text(face = \"bold\")\n  )\n\n# Convert the frequency polygon to an interactive plot\ninteractive_frequency_polygon &lt;- ggplotly(frequency_polygon)\n\n# Show the interactive frequency polygon\ninteractive_frequency_polygon\n\n\n\n\n\n\n# Load required libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\nlibrary(readr)  # Ensure you have the readr library to read CSV files\n\n# Read the data\ndata &lt;- read_csv(\n  \"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Website project\\\\website\\\\suicide-rates-all.csv\",\n  col_types = cols(\n    Country = col_character(),\n    Code = col_character(),\n    Year = col_double(),\n    `suscide-rate` = col_double()\n  )\n)\n\n# Step 1: Find top 5 countries with highest average suicide rates\ntop_5_countries &lt;- data %&gt;%\n  group_by(Country) %&gt;%\n  summarise(avg_suicide_rate = round(mean(`suscide-rate`, na.rm = TRUE), 2), .groups = \"drop\") %&gt;%\n  top_n(5, wt = avg_suicide_rate) %&gt;%\n  arrange(desc(avg_suicide_rate))\n\n# Step 2: Filter the original data to keep only the top 5 countries\ndata_top_countries &lt;- data %&gt;%\n  filter(Country %in% top_5_countries$Country)\n\n# Step 3: Find the top 5 unique years with highest suicide rates across all top countries\ntop_years &lt;- data_top_countries %&gt;%\n  arrange(desc(`suscide-rate`)) %&gt;%\n  distinct(Year, .keep_all = TRUE) %&gt;%  # Keep only unique years\n  slice_head(n = 5) %&gt;%  # Get top 5 unique years\n  pull(Year)  # Extract the years\n\n# Step 4: Filter the data for the top 5 countries and the selected top years\ntop_5_years_data &lt;- data_top_countries %&gt;%\n  filter(Year %in% top_years) %&gt;%\n  group_by(Country, Year) %&gt;%\n  summarise(`suscide-rate` = round(mean(`suscide-rate`, na.rm = TRUE), 2), .groups = \"drop\") %&gt;%\n  arrange(Country, desc(`suscide-rate`))  # Sort for final presentation\n\n# Step 5: Expand the custom color palette to match the number of unique years\nunique_years &lt;- length(unique(top_5_years_data$Year))\ncolor_palette &lt;- colorRampPalette(c(\"orange\", \"red\"))(unique_years)\n\n# Step 6: Create the plot using ggplot\nplot &lt;- ggplot(top_5_years_data, aes(x = Country, y = `suscide-rate`, fill = factor(Year))) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  coord_flip() +\n  scale_fill_manual(values = color_palette, name = \"Year\") +  # Use the custom palette\n  labs(title = \"Top 5 Years with Highest Suicide Rates in Top 5 Countries\", \n       x = \"Country\", \n       y = \"Suicide Rate per 100,000\") +\n  theme_minimal()\n\n# Step 7: Convert the plot to an interactive plot using plotly\n# Use sprintf to format the suicide rates to 2 decimal places in the tooltip\ninteractive_plot &lt;- ggplotly(plot) %&gt;%\n  style(hoverinfo = \"text\", \n        text = paste(\"Country: \", top_5_years_data$Country, \"&lt;br&gt;\",\n                     \"Year: \", top_5_years_data$Year, \"&lt;br&gt;\",\n                     \"Suicide Rate: \", sprintf(\"%.2f\", top_5_years_data$`suscide-rate`)))\n\n# Show the interactive plot\ninteractive_plot\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(rnaturalearth)\nlibrary(sf)\n\n# Read the data\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(rnaturalearth)\nlibrary(sf)\n\n# Read the data\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Website project\\\\website\\\\suicide-rates-by-age.csv\")\n\n# Clean column names for easier access\ncolnames(data) &lt;- c(\"Entity\", \"Code\", \"Year\", \n                    \"Death_rate_15_19\", \"Death_rate_20_24\", \"Death_rate_25_29\", \n                    \"Death_rate_30_34\", \"Death_rate_35_39\", \"Death_rate_40_44\", \n                    \"Death_rate_45_49\", \"Death_rate_50_54\", \"Death_rate_55_59\", \n                    \"Death_rate_60_64\", \"Death_rate_65_69\", \"Death_rate_70_74\", \n                    \"Death_rate_75_79\", \"Death_rate_80_84\", \"Death_rate_over_85\")\n\n# Filter for the 15-19 age group and calculate the sum of averages across all years for each country\ndata_summary &lt;- data %&gt;%\n  filter(!is.na(Death_rate_15_19)) %&gt;%\n  group_by(Entity) %&gt;%\n  summarise(Average_suscide_rate = sum(Death_rate_15_19, na.rm = TRUE), .groups = \"drop\") %&gt;%\n  mutate(Average_suscide_rate = round(Average_suscide_rate, 2))  # Round to 2 decimals\n\n# Load the world map using the rnaturalearth package\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# Join suicide data with the world map\nworld_data &lt;- world %&gt;%\n  left_join(data_summary, by = c(\"name\" = \"Entity\"))\n\n# Plot the map using ggplot with shades of red\np &lt;- ggplot(world_data) +\n  geom_sf(aes(fill = Average_suscide_rate, text = name), color = \"white\") +\n  scale_fill_gradient(low = \"#ffcccc\", high = \"#990000\", na.value = \"grey50\", \n                      name = \"Suicide Rate per 100,000\") +\n  theme_void() +\n  labs(title = \"Average Suicide Rates for Age 15-19 over the Years\")\n\n# Convert ggplot to an interactive plotly map\ninteractive_map &lt;- ggplotly(p, tooltip = c(\"text\", \"fill\"))\n\n# Show the interactive map\ninteractive_map\n\n\n\n\n\n\n# Load necessary libraries\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(plotly)\n\n# Load the data\ndata &lt;- read.csv(\"C:\\\\Users\\\\Venkata\\\\Desktop\\\\stat1\\\\Website project\\\\website\\\\suicide-rates-by-age.csv\")\n\n# Clean column names for easier access\ncolnames(data) &lt;- c(\"Entity\", \"Code\", \"Year\", \n                    \"Death_rate_15_19\", \"Death_rate_20_24\", \"Death_rate_25_29\", \n                    \"Death_rate_30_34\", \"Death_rate_35_39\", \"Death_rate_40_44\", \n                    \"Death_rate_45_49\", \"Death_rate_50_54\", \"Death_rate_55_59\", \n                    \"Death_rate_60_64\", \"Death_rate_65_69\", \"Death_rate_70_74\", \n                    \"Death_rate_75_79\", \"Death_rate_80_84\", \"Death_rate_over_85\")\n\n# Filter the data for the 15-19 age group and remove missing values\ndata_filtered &lt;- data %&gt;%\n  filter(!is.na(Death_rate_15_19))\n\n# Calculate the average suicide rate for each country across all years\navg_suicide_rate &lt;- data_filtered %&gt;%\n  group_by(Entity) %&gt;%\n  summarise(suscide_Rate = round(mean(Death_rate_15_19, na.rm = TRUE),2)) %&gt;%\n  arrange(desc(suscide_Rate))%&gt;%\n  top_n(20, suscide_Rate)\n\n\n\n# Create the bar plot with ggplot\nbar_plot &lt;- ggplot(avg_suicide_rate, aes(x = reorder(Entity, suscide_Rate), \n                                           y = suscide_Rate, \n                                           fill = suscide_Rate)) +\n  geom_bar(stat = \"identity\") +\n  coord_flip() +  # Flip coordinates to make country names readable\n  scale_fill_gradient(low = \"#ffcccc\", high = \"#990000\") +  # Light red to dark red gradient\n  labs(title = \"Average Suicide Rate for Age 15-19 (All Years)\", \n       x = \"Country\", \n       y = \"Average Suicide Rate per 100,000 Population\") +\n  theme_minimal() +\n  theme(axis.text.y = element_text(angle = 0, hjust = 1, size = 8)) +  # Adjust text angle and size\n  theme(plot.margin = margin(10, 10, 10, 40))  # Adjust margins\n\n# Convert to an interactive plot with plotly\ninteractive_plot &lt;- ggplotly(bar_plot, tooltip = c(\"suscide_Rate\"))\n\n# Show the interactive plot\ninteractive_plot"
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "References",
    "section": "",
    "text": "The data and insights used in this project are sourced from the following:\nData CSV: - Download file1.csv - Download file2.csv casualty_statistics.csv\nClick the links above to download the CSV files used for the analysis.\n\nReferences\n[1] “Suicide,” Our World in Data. [Online]. Available: https://ourworldindata.org/suicide. [Accessed: Oct. 8, 2024].\n[2] Department for Transport, “Road Safety Data,” data.gov.uk, [Online]. Available: https://www.data.gov.uk/dataset/cb7ae6f0-4be6-49359277-47e5ce24a11f/road-safety-data. [Accessed: Dec. 11, 2024].\n‌[3]“Creating websites with Quarto and GitHub,” YouTube. http://www.youtube.com/playlist?list=PLkrJrLs7xfbXcEKhTCKRSr2VXH4yiBeXo (accessed Dec. 11, 2024).\nFor more detailed information, please refer to the original publication."
  }
]